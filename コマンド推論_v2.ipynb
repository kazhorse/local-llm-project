{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d10276a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f2a459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (12239, 384)\n",
      "chunks shape: (12239, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_idx</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>id</th>\n",
       "      <th>midashi_1</th>\n",
       "      <th>kaisai_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ï¼‘ï¼èª¿æŸ»ç›®çš„ã¨èª¿æŸ»æ¦‚è¦ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼“ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼”ï¼”ï¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ¡ˆï¼‰ï¼ˆå‚è€ƒï¼‰ï½—ï½‡ã‚³ãƒ¡ãƒ³ãƒˆ...</td>\n",
       "      <td>26317</td>\n",
       "      <td>æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)</td>\n",
       "      <td>å¹³æˆ25å¹´3æœˆ4æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ãŸæ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸâ‘ â‘¡â‘¢æ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸãƒ™ãƒ³ãƒˆç®¡ã‚¹ãƒªãƒ¼ãƒ–ç«¯éƒ¨ã‚µãƒ³ãƒ‰ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒ‰ãƒ¬...</td>\n",
       "      <td>26317</td>\n",
       "      <td>æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)</td>\n",
       "      <td>å¹³æˆ25å¹´3æœˆ4æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼‘å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼‘ï¼–æ—¥ï¼ˆç«ï¼‰ï¼™...</td>\n",
       "      <td>34305</td>\n",
       "      <td>ç¬¬1å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ16æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼’å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼“æ—¥ï¼ˆç«ï¼‰ï¼‘...</td>\n",
       "      <td>34313</td>\n",
       "      <td>ç¬¬2å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ23æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼“å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼•æ—¥ï¼ˆæœ¨ï¼‰ï¼‘...</td>\n",
       "      <td>34321</td>\n",
       "      <td>ç¬¬3å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ25æ—¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_idx  chunk_id                                         chunk_text  \\\n",
       "0        0         0  ï¼‘ï¼èª¿æŸ»ç›®çš„ã¨èª¿æŸ»æ¦‚è¦ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼“ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼”ï¼”ï¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ¡ˆï¼‰ï¼ˆå‚è€ƒï¼‰ï½—ï½‡ã‚³ãƒ¡ãƒ³ãƒˆ...   \n",
       "1        0         1  ãŸæ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸâ‘ â‘¡â‘¢æ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸãƒ™ãƒ³ãƒˆç®¡ã‚¹ãƒªãƒ¼ãƒ–ç«¯éƒ¨ã‚µãƒ³ãƒ‰ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒ‰ãƒ¬...   \n",
       "2        1         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼‘å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼‘ï¼–æ—¥ï¼ˆç«ï¼‰ï¼™...   \n",
       "3        2         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼’å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼“æ—¥ï¼ˆç«ï¼‰ï¼‘...   \n",
       "4        3         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼“å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼•æ—¥ï¼ˆæœ¨ï¼‰ï¼‘...   \n",
       "\n",
       "      id                  midashi_1 kaisai_date  \n",
       "0  26317      æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)   å¹³æˆ25å¹´3æœˆ4æ—¥  \n",
       "1  26317      æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)   å¹³æˆ25å¹´3æœˆ4æ—¥  \n",
       "2  34305  ç¬¬1å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ16æ—¥  \n",
       "3  34313  ç¬¬2å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ23æ—¥  \n",
       "4  34321  ç¬¬3å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ25æ—¥  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ã•ã£ãã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "embeddings = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "\n",
    "print(\"embeddings shape:\", embeddings.shape)\n",
    "print(\"chunks shape:\", chunks.shape)\n",
    "chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb5c7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_index():\n",
    "    chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "    embeddings = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "    return chunks, embeddings\n",
    "\n",
    "df_chunks_loaded, embs_loaded = load_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284cb96",
   "metadata": {},
   "source": [
    "è³ªå•æ–‡ã‚·ã‚½ãƒ¼ãƒ©ã‚¹æ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da4908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_thesaurus(text: str) -> str:\n",
    "    \"\"\"\n",
    "    è³ªå•æ–‡ãªã©ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã€ã‚·ã‚½ãƒ¼ãƒ©ã‚¹å¤‰æ›ã‚’é©ç”¨ã€‚\n",
    "    - ä¼Šæ–¹ç™ºé›»æ‰€3å·/ï¼“å· â†’ ä¼Šæ–¹ç™ºé›»æ‰€3å·æ©Ÿ/ï¼“å·æ©Ÿï¼ˆæ—¢ã«å·æ©Ÿãªã‚‰ä½•ã‚‚ã—ãªã„ï¼‰\n",
    "    - ãƒ«ãƒ¼ãƒ«ã‚’å¢—ã‚„ã—ãŸã„å ´åˆã¯ PATTERNS ã« (pattern, repl) ã‚’è¿½åŠ \n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return text\n",
    "\n",
    "    # ã“ã“ã«ç½®æ›ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¦ã„ãï¼ˆä¸Šã‹ã‚‰é †ã«é©ç”¨ï¼‰\n",
    "    PATTERNS = [\n",
    "        # ä¼Šæ–¹ç™ºé›»æ‰€ + (3|ï¼“) + å·  â†’  â€¦ å·æ©Ÿ\n",
    "        # ã™ã§ã«ã€Œå·æ©Ÿã€ã«ãªã£ã¦ã„ãŸã‚‰ç½®æ›ã—ãªã„ãŸã‚ã« (?!æ©Ÿ) ã‚’å…¥ã‚Œã‚‹\n",
    "        (\n",
    "            r\"(ä¼Šæ–¹ç™ºé›»æ‰€)\\s*((?:3|ï¼“))\\s*å·(?!æ©Ÿ)\",\n",
    "            r\"\\1\\2å·æ©Ÿ\",\n",
    "        ),\n",
    "        # å¿…è¦ãªã‚‰åˆ¥å·æ©Ÿã‚„åˆ¥åŸç™ºã‚‚ã“ã“ã«ãƒ«ãƒ¼ãƒ«è¿½åŠ \n",
    "        # ä¾‹: å·å†…åŸç™º(1|ï¼‘)å· â†’ å·å†…åŸç™º(1|ï¼‘)å·æ©Ÿ\n",
    "        # (r\"(å·å†…åŸç™º)\\s*((?:1|ï¼‘))\\s*å·(?!æ©Ÿ)\", r\"\\1\\2å·æ©Ÿ\"),\n",
    "    ]\n",
    "\n",
    "    out = text\n",
    "    for pat, repl in PATTERNS:\n",
    "        out = re.sub(pat, repl, out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c2e6c",
   "metadata": {},
   "source": [
    "HyDEæ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9cee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ====== è¨­å®š ======\n",
    "MIN_LEN = 50  # æ–‡å­—æ•°ã—ãã„å€¤\n",
    "\n",
    "# LLMï¼ˆLM Studioï¼‰ã® OpenAIäº’æ›API\n",
    "LLM_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "LLM_MODEL = \"qwen/qwen3-vl-8b\"\n",
    "# ==================\n",
    "\n",
    "\n",
    "def count_chars(text: str) -> int:\n",
    "    \"\"\"Pythonã®len()ã§æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå…¨è§’/åŠè§’ã¨ã‚‚ã«1ã‚«ã‚¦ãƒ³ãƒˆï¼‰ã€‚\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def build_hyde_prompt(question: str) -> str:\n",
    "    \"\"\"HyDE ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\"\"\"\n",
    "    return (\n",
    "        \"ã‚ãªãŸã¯è³ªå•ã‚’ç†è§£ã—ã€èƒŒæ™¯ã‚„å‰æã€æ¡ä»¶ã€å…·ä½“ä¾‹ã‚’è£œã£ã¦ã€\"\n",
    "        \"å†…å®¹ã‚’è‡ªç„¶ã«æ‹¡å¼µã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\\n\\n\"\n",
    "        f\"ã€è³ªå•ã€‘\\n{question}\\n\\n\"\n",
    "        \"ã€æŒ‡ç¤ºã€‘\\n\"\n",
    "        \"ãƒ»ä¸Šã®è³ªå•ã‚’ã€æ„å‘³ã‚’å¤‰ãˆãšã«50æ–‡å­—ä»¥ä¸Šã¸è‡ªç„¶ã«æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "        \"ãƒ»è¿½åŠ ã®æ¡ä»¶ã€èƒŒæ™¯ã€å…·ä½“ä¾‹ã€ç›®çš„ã€åˆ¶ç´„ãªã©ã‚’è‡ªç„¶ã«åŠ ãˆã¦ãã ã•ã„ã€‚\\n\"\n",
    "        \"ãƒ»å‡ºåŠ›ã¯1ã¤ã®é•·ã„è³ªå•æ–‡ï¼ˆã¾ãŸã¯ä¾é ¼æ–‡ï¼‰ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\"\n",
    "    )\n",
    "\n",
    "\n",
    "def hyde_expand_with_local_llm(question: str) -> Optional[str]:\n",
    "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ« LLMï¼ˆLM Studioï¼‰ã§ HyDE æ‹¡å¼µ\"\"\"\n",
    "    try:\n",
    "        prompt = build_hyde_prompt(question)\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯ä¸å¯§ã§ç°¡æ½”ãªæ—¥æœ¬èªã®æ–‡ç« ä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "\n",
    "        resp = requests.post(LLM_URL, json=payload, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        expanded = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return expanded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLMjp çµŒç”±ã®HyDEæ‹¡å¼µã«å¤±æ•—:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def hyde_expand_fallback(question: str, min_len: int = MIN_LEN) -> str:\n",
    "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒå¿œç­”ã—ãªã„ã¨ãã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\"\"\"\n",
    "    base = question.strip()\n",
    "    addon = \"ï¼ˆèƒŒæ™¯ã‚„ç›®çš„ã€å‰ææ¡ä»¶ã€å…·ä½“ä¾‹ã‚’è£œè¶³ã—ã¦æ˜ç¢ºåŒ–ã—ã¦ãã ã•ã„ï¼‰\"\n",
    "    candidate = f\"{base}ã€‚{addon}\"\n",
    "\n",
    "    if len(candidate) < min_len:\n",
    "        need = min_len - len(candidate)\n",
    "        candidate += \" è©³ç´°ã‚’å«ã‚ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\" + (\"ã€‚\" * max(0, need - 12))\n",
    "\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def maybe_hyde_expand(question: str, min_len: int = MIN_LEN) -> Tuple[str, int, bool]:\n",
    "    \"\"\"\n",
    "    HyDE æ‹¡å¼µæœ¬ä½“\n",
    "    - æ–‡å­—æ•°50ä»¥ä¸Š â†’ ä½•ã‚‚ã—ãªã„\n",
    "    - æ–‡å­—æ•°50æœªæº€ â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMã§æ‹¡å¼µï¼ˆå¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "    \"\"\"\n",
    "    n = count_chars(question)\n",
    "\n",
    "    if n >= min_len:\n",
    "        return question, n, False\n",
    "\n",
    "    expanded = hyde_expand_with_local_llm(question)\n",
    "    if not expanded:\n",
    "        expanded = hyde_expand_fallback(question, min_len=min_len)\n",
    "\n",
    "    return expanded, count_chars(expanded), True\n",
    "\n",
    "\n",
    "def preprocess_question_with_hyde_then_tokens(question: str):\n",
    "    \"\"\"ä»–å‡¦ç†ã«ã¤ãªã HyDE æ‹¡å¼µâ†’è³ªå•è¿”å´\"\"\"\n",
    "    expanded_question, _, _ = maybe_hyde_expand(question, min_len=MIN_LEN)\n",
    "    return expanded_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec63f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã“ã‚“ã«ã¡ã¯ï¼ãƒ­ãƒ¼ã‚«ãƒ« LLM ã®æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã­ ğŸ˜Š  \n",
      "ï¼ˆâ€» å®Ÿéš›ã®ãƒ­ãƒ¼ã‚«ãƒ« LLM ãŒèµ·å‹•ã—ã¦ã„ã‚‹ç’°å¢ƒã§ãªã‘ã‚Œã°ã€ã“ã®å¿œç­”ã¯ã€Œä»®æƒ³çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆã€ã«ãªã‚Šã¾ã™ï¼‰\n",
      "\n",
      "---\n",
      "\n",
      "âœ… **æ¥ç¶šãƒ†ã‚¹ãƒˆçµæœï¼š**  \n",
      "**æ¥ç¶šæˆåŠŸï¼**  \n",
      "ï¼ˆâ€» æœ¬ä½“ã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã«å­˜åœ¨ã—ãªã„å ´åˆã€ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¥ç¶šã€ã§ã™ï¼‰\n",
      "\n",
      "---\n",
      "\n",
      "ğŸ” **ãƒ†ã‚¹ãƒˆå†…å®¹ï¼š**  \n",
      "- æ¥ç¶šçŠ¶æ…‹ï¼šæ­£å¸¸  \n",
      "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹é€Ÿåº¦ï¼šç¬æ™‚ï¼ˆä»®æƒ³ç’°å¢ƒã§ã¯0msï¼‰  \n",
      "- ãƒ¢ãƒ‡ãƒ«åï¼šæœªæŒ‡å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã«ãƒ¢ãƒ‡ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿æœ‰åŠ¹ï¼‰  \n",
      "- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼š0%ï¼ˆä»®æƒ³ç’°å¢ƒã§ã¯ç„¡è¦–ï¼‰\n",
      "\n",
      "---\n",
      "\n",
      "ğŸ’¡ **æ¬¡ã«ã§ãã‚‹ã“ã¨ï¼š**  \n",
      "- ãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚„ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ã€å®Ÿéš›ã®æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã™ï¼  \n",
      "- ä¾‹ï¼š`llama3-7b`ã€`Llama-3-8B`ã€`Qwen2-7B`ã€`Phi-3` ãªã©\n",
      "\n",
      "---\n",
      "\n",
      "ğŸš€ **ãƒ†ã‚¹ãƒˆå®Œäº†ï¼**  \n",
      "ãƒ­ãƒ¼ã‚«ãƒ« LLM ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã‹ã€ã”ç¢ºèªãã ã•ã„ã€‚  \n",
      "ä½•ã‹è¨­å®šã‚„ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ã”ç›¸è«‡ãã ã•ã„ï¼\n",
      "\n",
      "---\n",
      "\n",
      "ï¼ˆâ€» ã“ã®å¿œç­”ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ« LLM ãŒå®Ÿéš›ã«å­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ã€æ¥ç¶šãƒ†ã‚¹ãƒˆã®ã€Œå½¢å¼çš„ã€ãªå¯¾å¿œã¨ã—ã¦æä¾›ã—ã¦ã„ã¾ã™ï¼‰\n",
      "\n",
      "---\n",
      "\n",
      "ä½•ã‹ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚„ç’°å¢ƒã«ã¤ã„ã¦ãƒ†ã‚¹ãƒˆã—ãŸã„å ´åˆã¯ã€ãŠçŸ¥ã‚‰ã›ãã ã•ã„ï¼  \n",
      "ä¾‹ï¼š  \n",
      "> ã€Œç§ã®PCã«Llama3ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æ¥ç¶šãƒ†ã‚¹ãƒˆã—ã¦ã€  \n",
      "> ã€ŒQwen2-7Bã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã—ã¦ã„ã‚‹ã®ã§ã€æ¥ç¶šç¢ºèªã€\n",
      "\n",
      "ãŠæ‰‹ä¼ã„ã—ã¾ã™ï¼ ğŸ¤–ğŸ’¬\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def llm_chat(messages, max_tokens=1024, temperature=0.2):\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(LLM_URL, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(llm_chat([{\"role\": \"user\", \"content\": \"ãƒ­ãƒ¼ã‚«ãƒ« LLM æ¥ç¶šãƒ†ã‚¹ãƒˆã§ã™ã€‚\"}]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b7d25",
   "metadata": {},
   "source": [
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b12990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from janome.tokenizer import Tokenizer\n",
    "# from typing import List\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # --- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼šåè©ï¼‹é€£ç¶šåè©ã®è¤‡åˆèªï¼ˆn>=2 ã¯ \"_\" é€£çµï¼‰ ---\n",
    "# _t = Tokenizer()\n",
    "\n",
    "# def extract_nouns_and_compounds(text: str) -> List[str]:\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return []\n",
    "#     nouns, spans, cur = [], [], []\n",
    "#     for tok in _t.tokenize(text):\n",
    "#         base = tok.base_form if tok.base_form != \"*\" else tok.surface\n",
    "#         pos0 = tok.part_of_speech.split(\",\")[0]\n",
    "#         if pos0 == \"åè©\" and base:\n",
    "#             nouns.append(base)\n",
    "#             cur.append(base)\n",
    "#         else:\n",
    "#             if cur:\n",
    "#                 spans.append(cur)\n",
    "#                 cur = []\n",
    "#     if cur:\n",
    "#         spans.append(cur)\n",
    "\n",
    "#     # åè©ãŒé€£ç¶šã—ã¦ã„ã‚‹éƒ¨åˆ†ã‹ã‚‰è¤‡åˆèªã‚’ä½œã‚‹\n",
    "#     compounds = []\n",
    "#     for span in spans:\n",
    "#         if len(span) >= 2:\n",
    "#             for L in range(2, len(span) + 1):\n",
    "#                 for i in range(len(span) - L + 1):\n",
    "#                     compounds.append(\"_\".join(span[i:i+L]))\n",
    "\n",
    "#     return nouns + compounds\n",
    "\n",
    "\n",
    "# # --- TF-IDFã§ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º ---\n",
    "# def tfidf_top_k(tokens_list: List[List[str]], k: int = 5) -> List[str]:\n",
    "#     vec = TfidfVectorizer(\n",
    "#         analyzer=\"word\",\n",
    "#         tokenizer=lambda x: x,\n",
    "#         preprocessor=lambda x: x,\n",
    "#         token_pattern=None,\n",
    "#         ngram_range=(1, 1),\n",
    "#         min_df=1,\n",
    "#     )\n",
    "#     X = vec.fit_transform(tokens_list)\n",
    "#     feats = np.array(vec.get_feature_names_out())\n",
    "\n",
    "#     row = X.getrow(0)\n",
    "#     if row.nnz == 0:\n",
    "#         return []\n",
    "\n",
    "#     idx = row.indices\n",
    "#     data = row.data\n",
    "#     top = idx[np.argsort(-data)[:k]]\n",
    "\n",
    "#     return feats[top].tolist()\n",
    "\n",
    "\n",
    "# def extract_keywords_tfidf(text: str, top_k: int = 5) -> List[str]:\n",
    "#     tokens = extract_nouns_and_compounds(text)\n",
    "#     if not tokens:\n",
    "#         return []\n",
    "#     return tfidf_top_k([tokens], k=top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fe50f",
   "metadata": {},
   "source": [
    "#LLMã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "697f4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords_llm(question: str, max_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    ãƒ­ãƒ¼ã‚«ãƒ« LLMï¼ˆOpenAI API äº’æ›ï¼‰ã§è³ªå•ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã€‚\n",
    "    å¿…ãš JSON å½¢å¼ã®é…åˆ—ã ã‘ã‚’è¿”ã•ã›ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "ä»¥ä¸‹ã®è³ªå•æ–‡ã‹ã‚‰ã€é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ã€è³ªå•ã€‘\n",
    "{question}\n",
    "\n",
    "ã€è¦ä»¶ã€‘\n",
    "- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’è¿”ã™\n",
    "- èª¬æ˜æ–‡ã‚„å‰ç½®ãã¯ç¦æ­¢\n",
    "- JSON ã®é…åˆ—ã®ã¿ã‚’è¿”ã™\n",
    "- {max_k} å€‹ä»¥å†…ã«ã™ã‚‹\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(LLM_URL, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        # JSON ä»¥å¤–ãŒæ··ã˜ã£ã¦ã„ãŸã‚‰é™¤å»ã™ã‚‹å®‰å…¨å‡¦ç†\n",
    "        import json\n",
    "        keywords = json.loads(content)\n",
    "\n",
    "        # å¿µã®ãŸã‚æ–‡å­—åˆ—ã ã‘ã«ã™ã‚‹\n",
    "        keywords = [k for k in keywords if isinstance(k, str)]\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM keyword generation failed:\", e)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e994a9e",
   "metadata": {},
   "source": [
    "#oræ¤œç´¢â†’andæ¤œç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#andæ¤œç´¢\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "# def keyword_mask_on_chunks(df_chunks: pd.DataFrame, keywords):\n",
    "#     if not keywords:\n",
    "#         return np.ones(len(df_chunks), dtype=bool)\n",
    "\n",
    "#     s = df_chunks[\"chunk_text\"].fillna(\"\")\n",
    "#     mask = np.ones(len(df_chunks), dtype=bool)\n",
    "\n",
    "#     # ã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã ã‘ Trueï¼ˆAND æ¤œç´¢ï¼‰\n",
    "#     for kw in keywords:\n",
    "#         mask &= s.str.contains(re.escape(kw), regex=True)\n",
    "\n",
    "#     return mask\n",
    "\n",
    "# oræ¤œç´¢\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def keyword_mask_on_chunks(df_chunks: pd.DataFrame, keywords):\n",
    "    if not keywords:\n",
    "        return np.ones(len(df_chunks), dtype=bool)\n",
    "    pattern = \"|\".join(map(re.escape, keywords))\n",
    "    return df_chunks[\"chunk_text\"].fillna(\"\").str.contains(pattern, regex=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4194ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "def build_faiss_ip(embs: np.ndarray):\n",
    "    if embs.size == 0:\n",
    "        return None\n",
    "    d = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)     # normalize_embeddings=True ãªã®ã§å†…ç©=ã‚³ã‚µã‚¤ãƒ³\n",
    "    index.add(embs.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "def vector_search_in_filtered_chunks(df_chunks, embs, encoder, query, pre_mask=None, keywords=None, top_k=5):\n",
    "    # ã©ã¡ã‚‰ã‹æŒ‡å®šï¼špre_maskå„ªå…ˆ\n",
    "    if pre_mask is None:\n",
    "        pre_mask = keyword_mask_on_chunks(df_chunks, keywords or [])\n",
    "    if pre_mask.sum() == 0:\n",
    "        return df_chunks.head(0).assign(score=[])\n",
    "\n",
    "    df_f = df_chunks.loc[pre_mask].reset_index(drop=True)\n",
    "    embs_f = embs[pre_mask]\n",
    "\n",
    "    index = build_faiss_ip(embs_f)\n",
    "    if index is None:\n",
    "        return df_chunks.head(0).assign(score=[])\n",
    "\n",
    "    q = encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "    scores, idxs = index.search(q, top_k)\n",
    "    scores, idxs = scores[0], idxs[0]\n",
    "\n",
    "    hits = []\n",
    "    for rank, (i_sub, s) in enumerate(zip(idxs, scores), 1):\n",
    "        if i_sub < 0:\n",
    "            continue\n",
    "        row = df_f.iloc[int(i_sub)].copy()\n",
    "        row[\"rank\"] = rank\n",
    "        row[\"score\"] = float(s)\n",
    "        hits.append(row)\n",
    "    return pd.DataFrame(hits).sort_values(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "193deaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š chunks=12239, embs=(12239, 384)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼†ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "def load_index():\n",
    "    df_chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "    embs = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    print(f\"ğŸ“š chunks={len(df_chunks)}, embs={embs.shape}\")\n",
    "    return df_chunks, embs, encoder\n",
    "\n",
    "df_chunks, embs, encoder = load_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e570e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(question: str, top_k: int = 5):\n",
    "    # 1) è³ªå•å‰å‡¦ç†\n",
    "    q = apply_thesaurus(question)\n",
    "    q = preprocess_question_with_hyde_then_tokens(q)\n",
    "\n",
    "\n",
    "\n",
    "    #æ—§ï¼šTF-IDF\n",
    "    #keywords = extract_keywords_tfidf(q, top_k=3)\n",
    "    # æ–°ï¼šLLMãƒ™ãƒ¼ã‚¹\n",
    "    keywords = generate_keywords_llm(q, max_k=3)\n",
    "    #keywords = [\"æ®‹ç•™ç†±é™¤å»ç³»\"]\n",
    "\n",
    "\n",
    "    print(\"ğŸ“ question:\", question)\n",
    "    print(\"ğŸ“ question (HyDEå¾Œ):\", q)\n",
    "    print(\"ğŸ”‘ keywords:\", keywords)\n",
    "\n",
    "    # 2) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒãƒ£ãƒ³ã‚¯çµã‚Šè¾¼ã¿\n",
    "    mask = keyword_mask_on_chunks(df_chunks, keywords)\n",
    "    print(f\"ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒãƒ£ãƒ³ã‚¯æ•°: {mask.sum()} / {len(df_chunks)}\")\n",
    "\n",
    "    # 3) ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢\n",
    "    results = vector_search_in_filtered_chunks(\n",
    "        df_chunks=df_chunks,\n",
    "        embs=embs,\n",
    "        encoder=encoder,\n",
    "        query=q,\n",
    "        pre_mask=mask,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    cols = [\"rank\", \"score\", \"id\", \"midashi_1\", \"doc_idx\", \"chunk_id\", \"chunk_text\", \"kaisai_date\"]\n",
    "    return results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b755d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def build_context_from_results(results: pd.DataFrame, max_chars: int = 4000) -> str:\n",
    "    \"\"\"\n",
    "    search() ã®çµæœ DataFrame ã‹ã‚‰ã€LLM ã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’ä½œã‚‹ã€‚\n",
    "    é•·ããªã‚Šã™ããªã„ã‚ˆã†ã« max_chars ã§æ‰“ã¡åˆ‡ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total_len = 0\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        header = f\"[doc_idx={row.get('doc_idx')}, chunk_id={row.get('chunk_id')}, title={row.get('title_1', '')}]\"\n",
    "        body = str(row[\"chunk_text\"])\n",
    "        block = header + \"\\n\" + textwrap.fill(body, 80) + \"\\n\"\n",
    "\n",
    "        if total_len + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total_len += len(block)\n",
    "\n",
    "    if not parts:\n",
    "        return \"(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ)\"\n",
    "\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "\n",
    "def answer_with_rag(question: str, top_k: int = 5, max_context_chars: int = 4000):\n",
    "    \"\"\"\n",
    "    ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆLLMjpï¼‰ã‚’åˆ©ç”¨ã—ãŸ RAG å›ç­”ç”Ÿæˆã€‚\n",
    "    1. search() ã§é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—\n",
    "    2. ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã« LLMjp ã§å›ç­”ã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    # 1) ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢\n",
    "    results = search(question, top_k=top_k)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return \"é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\", results\n",
    "\n",
    "    # 2) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ„ã¿ç«‹ã¦\n",
    "    context = build_context_from_results(results, max_chars=max_context_chars)\n",
    "\n",
    "    system_prompt = \"\"\"ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹åŸå­åŠ›ãƒ»å·¥å­¦ç³»ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n",
    "ä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã«å«ã¾ã‚Œã‚‹æƒ…å ±ã‚’æœ€å„ªå…ˆã—ã¦ä½¿ã„ã€\n",
    "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ã„ã¦ã„ãªã„ã“ã¨ã¯æ†¶æ¸¬ã§è£œã‚ãšã€\n",
    "ãã®å ´åˆã¯ã€Œè³‡æ–™ã‹ã‚‰ã¯èª­ã¿å–ã‚Œã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"# è³ªå•\n",
    "{question}\n",
    "\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ¤œç´¢çµæœï¼‰\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "    # LLMjpã‚’å‘¼ã¶\n",
    "    answer = llm_chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return answer.strip(), results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37e5ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ question: \n",
      "æ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å‚™ã®ã†ã¡ã€æºå¸¯å‹ç«¯æœ«ã§ã‚ã‚‹ãƒãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã©ã®ç¨‹åº¦é…å‚™ã•ã‚Œã¦ã„ã¾ã™ã‹ã€‚\n",
      "\n",
      "ğŸ“ question (HyDEå¾Œ): \n",
      "æ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å‚™ã®ã†ã¡ã€æºå¸¯å‹ç«¯æœ«ã§ã‚ã‚‹ãƒãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã©ã®ç¨‹åº¦é…å‚™ã•ã‚Œã¦ã„ã¾ã™ã‹ã€‚\n",
      "\n",
      "ğŸ”‘ keywords: ['æ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å‚™', 'æºå¸¯å‹ç«¯æœ«', 'ãƒãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³']\n",
      "ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒãƒ£ãƒ³ã‚¯æ•°: 0 / 12239\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['rank'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m q = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mæ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å‚™ã®ã†ã¡ã€æºå¸¯å‹ç«¯æœ«ã§ã‚ã‚‹ãƒãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã©ã®ç¨‹åº¦é…å‚™ã•ã‚Œã¦ã„ã¾ã™ã‹ã€‚\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m answer, used_chunks = \u001b[43manswer_with_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâ–¼ å›ç­”\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36manswer_with_rag\u001b[39m\u001b[34m(question, top_k, max_context_chars)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03mãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆLLMjpï¼‰ã‚’åˆ©ç”¨ã—ãŸ RAG å›ç­”ç”Ÿæˆã€‚\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m1. search() ã§é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m2. ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã« LLMjp ã§å›ç­”ã‚’ç”Ÿæˆ\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 1) ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m results = \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) == \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mé–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\u001b[39m\u001b[33m\"\u001b[39m, results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(question, top_k)\u001b[39m\n\u001b[32m     24\u001b[39m results = vector_search_in_filtered_chunks(\n\u001b[32m     25\u001b[39m     df_chunks=df_chunks,\n\u001b[32m     26\u001b[39m     embs=embs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     top_k=top_k,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m cols = [\u001b[33m\"\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmidashi_1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdoc_idx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkaisai_date\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ãƒ­ãƒ¼ã‚«ãƒ«LLM/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ãƒ­ãƒ¼ã‚«ãƒ«LLM/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ãƒ­ãƒ¼ã‚«ãƒ«LLM/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['rank'] not in index\""
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "æ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­å‚™ã®ã†ã¡ã€æºå¸¯å‹ç«¯æœ«ã§ã‚ã‚‹ãƒãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã©ã®ç¨‹åº¦é…å‚™ã•ã‚Œã¦ã„ã¾ã™ã‹ã€‚\n",
    "\"\"\"\n",
    "\n",
    "answer, used_chunks = answer_with_rag(q, top_k=5)\n",
    "\n",
    "print(\"â–¼ å›ç­”\\n\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nâ–¼ å‚ç…§ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ï¼ˆä¸Šä½ï¼‰\\n\")\n",
    "display(used_chunks[[\"rank\", \"score\", \"id\", \"midashi_1\", \"doc_idx\", \"chunk_id\", \"kaisai_date\"]].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
