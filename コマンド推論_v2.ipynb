{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10276a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2a459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape: (12239, 384)\n",
      "chunks shape: (12239, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_idx</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>id</th>\n",
       "      <th>midashi_1</th>\n",
       "      <th>kaisai_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ï¼‘ï¼èª¿æŸ»ç›®çš„ã¨èª¿æŸ»æ¦‚è¦ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼“ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼”ï¼”ï¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ¡ˆï¼‰ï¼ˆå‚è€ƒï¼‰ï½—ï½‡ã‚³ãƒ¡ãƒ³ãƒˆ...</td>\n",
       "      <td>26317</td>\n",
       "      <td>æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)</td>\n",
       "      <td>å¹³æˆ25å¹´3æœˆ4æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ãŸæ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸâ‘ â‘¡â‘¢æ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸãƒ™ãƒ³ãƒˆç®¡ã‚¹ãƒªãƒ¼ãƒ–ç«¯éƒ¨ã‚µãƒ³ãƒ‰ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒ‰ãƒ¬...</td>\n",
       "      <td>26317</td>\n",
       "      <td>æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)</td>\n",
       "      <td>å¹³æˆ25å¹´3æœˆ4æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼‘å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼‘ï¼–æ—¥ï¼ˆç«ï¼‰ï¼™...</td>\n",
       "      <td>34305</td>\n",
       "      <td>ç¬¬1å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ16æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼’å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼“æ—¥ï¼ˆç«ï¼‰ï¼‘...</td>\n",
       "      <td>34313</td>\n",
       "      <td>ç¬¬2å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ23æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼“å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼•æ—¥ï¼ˆæœ¨ï¼‰ï¼‘...</td>\n",
       "      <td>34321</td>\n",
       "      <td>ç¬¬3å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>å¹³æˆ25å¹´7æœˆ25æ—¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_idx  chunk_id                                         chunk_text  \\\n",
       "0        0         0  ï¼‘ï¼èª¿æŸ»ç›®çš„ã¨èª¿æŸ»æ¦‚è¦ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼“ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼”ï¼”ï¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ¡ˆï¼‰ï¼ˆå‚è€ƒï¼‰ï½—ï½‡ã‚³ãƒ¡ãƒ³ãƒˆ...   \n",
       "1        0         1  ãŸæ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸâ‘ â‘¡â‘¢æ¼æ°´ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸãƒ™ãƒ³ãƒˆç®¡ã‚¹ãƒªãƒ¼ãƒ–ç«¯éƒ¨ã‚µãƒ³ãƒ‰ã‚¯ãƒƒã‚·ãƒ§ãƒ³ãƒ‰ãƒ¬...   \n",
       "2        1         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼‘å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼‘ï¼–æ—¥ï¼ˆç«ï¼‰ï¼™...   \n",
       "3        2         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼’å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼“æ—¥ï¼ˆç«ï¼‰ï¼‘...   \n",
       "4        3         0  åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼“å›ï¼‰ï¼‘ï¼æ—¥æ™‚ï¼šå¹³æˆï¼’ï¼•å¹´ï¼—æœˆï¼’ï¼•æ—¥ï¼ˆæœ¨ï¼‰ï¼‘...   \n",
       "\n",
       "      id                  midashi_1 kaisai_date  \n",
       "0  26317      æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)   å¹³æˆ25å¹´3æœˆ4æ—¥  \n",
       "1  26317      æ±äº¬é›»åŠ›(æ ª)ç­‰ã¨ã®é¢è«‡(å¹³æˆ25å¹´3æœˆ)   å¹³æˆ25å¹´3æœˆ4æ—¥  \n",
       "2  34305  ç¬¬1å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ16æ—¥  \n",
       "3  34313  ç¬¬2å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ23æ—¥  \n",
       "4  34321  ç¬¬3å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ  å¹³æˆ25å¹´7æœˆ25æ—¥  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ã•ã£ãã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "embeddings = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "\n",
    "print(\"embeddings shape:\", embeddings.shape)\n",
    "print(\"chunks shape:\", chunks.shape)\n",
    "chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5c7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_index():\n",
    "    chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "    embeddings = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "    return chunks, embeddings\n",
    "\n",
    "df_chunks_loaded, embs_loaded = load_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284cb96",
   "metadata": {},
   "source": [
    "è³ªå•æ–‡ã‚·ã‚½ãƒ¼ãƒ©ã‚¹æ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_thesaurus(text: str) -> str:\n",
    "    \"\"\"\n",
    "    è³ªå•æ–‡ãªã©ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã€ã‚·ã‚½ãƒ¼ãƒ©ã‚¹å¤‰æ›ã‚’é©ç”¨ã€‚\n",
    "    - ä¼Šæ–¹ç™ºé›»æ‰€3å·/ï¼“å· â†’ ä¼Šæ–¹ç™ºé›»æ‰€3å·æ©Ÿ/ï¼“å·æ©Ÿï¼ˆæ—¢ã«å·æ©Ÿãªã‚‰ä½•ã‚‚ã—ãªã„ï¼‰\n",
    "    - ãƒ«ãƒ¼ãƒ«ã‚’å¢—ã‚„ã—ãŸã„å ´åˆã¯ PATTERNS ã« (pattern, repl) ã‚’è¿½åŠ \n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return text\n",
    "\n",
    "    # ã“ã“ã«ç½®æ›ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¦ã„ãï¼ˆä¸Šã‹ã‚‰é †ã«é©ç”¨ï¼‰\n",
    "    PATTERNS = [\n",
    "        # ä¼Šæ–¹ç™ºé›»æ‰€ + (3|ï¼“) + å·  â†’  â€¦ å·æ©Ÿ\n",
    "        # ã™ã§ã«ã€Œå·æ©Ÿã€ã«ãªã£ã¦ã„ãŸã‚‰ç½®æ›ã—ãªã„ãŸã‚ã« (?!æ©Ÿ) ã‚’å…¥ã‚Œã‚‹\n",
    "        (\n",
    "            r\"(ä¼Šæ–¹ç™ºé›»æ‰€)\\s*((?:3|ï¼“))\\s*å·(?!æ©Ÿ)\",\n",
    "            r\"\\1\\2å·æ©Ÿ\",\n",
    "        ),\n",
    "        # å¿…è¦ãªã‚‰åˆ¥å·æ©Ÿã‚„åˆ¥åŸç™ºã‚‚ã“ã“ã«ãƒ«ãƒ¼ãƒ«è¿½åŠ \n",
    "        # ä¾‹: å·å†…åŸç™º(1|ï¼‘)å· â†’ å·å†…åŸç™º(1|ï¼‘)å·æ©Ÿ\n",
    "        # (r\"(å·å†…åŸç™º)\\s*((?:1|ï¼‘))\\s*å·(?!æ©Ÿ)\", r\"\\1\\2å·æ©Ÿ\"),\n",
    "    ]\n",
    "\n",
    "    out = text\n",
    "    for pat, repl in PATTERNS:\n",
    "        out = re.sub(pat, repl, out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c2e6c",
   "metadata": {},
   "source": [
    "HyDEæ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ====== è¨­å®š ======\n",
    "MIN_LEN = 20  # æ–‡å­—æ•°ã—ãã„å€¤\n",
    "\n",
    "# LLMï¼ˆLM Studioï¼‰ã® OpenAIäº’æ›API\n",
    "LLM_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "LLM_MODEL = \"qwen/qwen3-vl-8b\"\n",
    "# ==================\n",
    "\n",
    "\n",
    "def count_chars(text: str) -> int:\n",
    "    \"\"\"Pythonã®len()ã§æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå…¨è§’/åŠè§’ã¨ã‚‚ã«1ã‚«ã‚¦ãƒ³ãƒˆï¼‰ã€‚\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def build_hyde_prompt(question: str) -> str:\n",
    "    \"\"\"HyDE ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\"\"\"\n",
    "    return (\n",
    "        \"ã‚ãªãŸã¯è³ªå•ã‚’ç†è§£ã—ã€èƒŒæ™¯ã‚„å‰æã€æ¡ä»¶ã€å…·ä½“ä¾‹ã‚’è£œã£ã¦ã€\"\n",
    "        \"å†…å®¹ã‚’è‡ªç„¶ã«æ‹¡å¼µã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\\n\\n\"\n",
    "        f\"ã€è³ªå•ã€‘\\n{question}\\n\\n\"\n",
    "        \"ã€æŒ‡ç¤ºã€‘\\n\"\n",
    "        \"ãƒ»ä¸Šã®è³ªå•ã‚’ã€æ„å‘³ã‚’å¤‰ãˆãšã«50æ–‡å­—ä»¥ä¸Šã¸è‡ªç„¶ã«æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "        \"ãƒ»è¿½åŠ ã®æ¡ä»¶ã€èƒŒæ™¯ã€å…·ä½“ä¾‹ã€ç›®çš„ã€åˆ¶ç´„ãªã©ã‚’è‡ªç„¶ã«åŠ ãˆã¦ãã ã•ã„ã€‚\\n\"\n",
    "        \"ãƒ»å‡ºåŠ›ã¯1ã¤ã®é•·ã„è³ªå•æ–‡ï¼ˆã¾ãŸã¯ä¾é ¼æ–‡ï¼‰ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\"\n",
    "    )\n",
    "\n",
    "\n",
    "def hyde_expand_with_local_llm(question: str) -> Optional[str]:\n",
    "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ« LLMï¼ˆLM Studioï¼‰ã§ HyDE æ‹¡å¼µ\"\"\"\n",
    "    try:\n",
    "        prompt = build_hyde_prompt(question)\n",
    "\n",
    "        payload = {\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯ä¸å¯§ã§ç°¡æ½”ãªæ—¥æœ¬èªã®æ–‡ç« ä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "\n",
    "        resp = requests.post(LLM_URL, json=payload, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        expanded = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return expanded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLMjp çµŒç”±ã®HyDEæ‹¡å¼µã«å¤±æ•—:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def hyde_expand_fallback(question: str, min_len: int = MIN_LEN) -> str:\n",
    "    \"\"\"ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒå¿œç­”ã—ãªã„ã¨ãã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\"\"\"\n",
    "    base = question.strip()\n",
    "    addon = \"ï¼ˆèƒŒæ™¯ã‚„ç›®çš„ã€å‰ææ¡ä»¶ã€å…·ä½“ä¾‹ã‚’è£œè¶³ã—ã¦æ˜ç¢ºåŒ–ã—ã¦ãã ã•ã„ï¼‰\"\n",
    "    candidate = f\"{base}ã€‚{addon}\"\n",
    "\n",
    "    if len(candidate) < min_len:\n",
    "        need = min_len - len(candidate)\n",
    "        candidate += \" è©³ç´°ã‚’å«ã‚ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\" + (\"ã€‚\" * max(0, need - 12))\n",
    "\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def maybe_hyde_expand(question: str, min_len: int = MIN_LEN) -> Tuple[str, int, bool]:\n",
    "    \"\"\"\n",
    "    HyDE æ‹¡å¼µæœ¬ä½“\n",
    "    - æ–‡å­—æ•°50ä»¥ä¸Š â†’ ä½•ã‚‚ã—ãªã„\n",
    "    - æ–‡å­—æ•°50æœªæº€ â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMã§æ‹¡å¼µï¼ˆå¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "    \"\"\"\n",
    "    n = count_chars(question)\n",
    "\n",
    "    if n >= min_len:\n",
    "        return question, n, False\n",
    "\n",
    "    expanded = hyde_expand_with_local_llm(question)\n",
    "    if not expanded:\n",
    "        expanded = hyde_expand_fallback(question, min_len=min_len)\n",
    "\n",
    "    return expanded, count_chars(expanded), True\n",
    "\n",
    "\n",
    "def preprocess_question_with_hyde_then_tokens(question: str):\n",
    "    \"\"\"ä»–å‡¦ç†ã«ã¤ãªã HyDE æ‹¡å¼µâ†’è³ªå•è¿”å´\"\"\"\n",
    "    expanded_question, _, _ = maybe_hyde_expand(question, min_len=MIN_LEN)\n",
    "    return expanded_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec63f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã“ã‚“ã«ã¡ã¯ï¼ãƒ­ãƒ¼ã‚«ãƒ« LLM æ¥ç¶šãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã­ ğŸ˜Š\n",
      "\n",
      "**ãƒ†ã‚¹ãƒˆå†…å®¹ï¼š**\n",
      "- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ LLM ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã‚‹ã‹ç¢ºèª\n",
      "- ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã£ã¦ãã‚‹ã‹ç¢ºèª\n",
      "\n",
      "**ãƒ†ã‚¹ãƒˆçµæœï¼š**\n",
      "âœ… æ¥ç¶šæˆåŠŸï¼  \n",
      "âœ… ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡æˆåŠŸ  \n",
      "âœ… æ­£å¸¸ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿”å´\n",
      "\n",
      "**ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š**  \n",
      "ã€Œãƒ­ãƒ¼ã‚«ãƒ« LLM ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼ã€\n",
      "\n",
      "---\n",
      "\n",
      "ä½•ã‹ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„è³ªå•ãŒã‚ã‚Œã°ã€ã„ã¤ã§ã‚‚ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚  \n",
      "ä¾‹ï¼š  \n",
      "- ã€Œã“ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ã€  \n",
      "- ã€Œã“ã®è³ªå•ã«ç­”ãˆãªã•ã„ã€  \n",
      "- ã€Œãƒ­ãƒ¼ã‚«ãƒ« LLM ã®è¨­å®šã‚’ç¢ºèªã—ãŸã„ã€\n",
      "\n",
      "ãŠæ°—è»½ã«ã©ã†ãï¼ ğŸš€\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def llm_chat(messages, max_tokens=1024, temperature=0.2):\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(LLM_URL, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆ\n",
    "print(llm_chat([{\"role\": \"user\", \"content\": \"ãƒ­ãƒ¼ã‚«ãƒ« LLM æ¥ç¶šãƒ†ã‚¹ãƒˆã§ã™ã€‚\"}]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b7d25",
   "metadata": {},
   "source": [
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from janome.tokenizer import Tokenizer\n",
    "# from typing import List\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # --- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼šåè©ï¼‹é€£ç¶šåè©ã®è¤‡åˆèªï¼ˆn>=2 ã¯ \"_\" é€£çµï¼‰ ---\n",
    "# _t = Tokenizer()\n",
    "\n",
    "# def extract_nouns_and_compounds(text: str) -> List[str]:\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return []\n",
    "#     nouns, spans, cur = [], [], []\n",
    "#     for tok in _t.tokenize(text):\n",
    "#         base = tok.base_form if tok.base_form != \"*\" else tok.surface\n",
    "#         pos0 = tok.part_of_speech.split(\",\")[0]\n",
    "#         if pos0 == \"åè©\" and base:\n",
    "#             nouns.append(base)\n",
    "#             cur.append(base)\n",
    "#         else:\n",
    "#             if cur:\n",
    "#                 spans.append(cur)\n",
    "#                 cur = []\n",
    "#     if cur:\n",
    "#         spans.append(cur)\n",
    "\n",
    "#     # åè©ãŒé€£ç¶šã—ã¦ã„ã‚‹éƒ¨åˆ†ã‹ã‚‰è¤‡åˆèªã‚’ä½œã‚‹\n",
    "#     compounds = []\n",
    "#     for span in spans:\n",
    "#         if len(span) >= 2:\n",
    "#             for L in range(2, len(span) + 1):\n",
    "#                 for i in range(len(span) - L + 1):\n",
    "#                     compounds.append(\"_\".join(span[i:i+L]))\n",
    "\n",
    "#     return nouns + compounds\n",
    "\n",
    "\n",
    "# # --- TF-IDFã§ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º ---\n",
    "# def tfidf_top_k(tokens_list: List[List[str]], k: int = 5) -> List[str]:\n",
    "#     vec = TfidfVectorizer(\n",
    "#         analyzer=\"word\",\n",
    "#         tokenizer=lambda x: x,\n",
    "#         preprocessor=lambda x: x,\n",
    "#         token_pattern=None,\n",
    "#         ngram_range=(1, 1),\n",
    "#         min_df=1,\n",
    "#     )\n",
    "#     X = vec.fit_transform(tokens_list)\n",
    "#     feats = np.array(vec.get_feature_names_out())\n",
    "\n",
    "#     row = X.getrow(0)\n",
    "#     if row.nnz == 0:\n",
    "#         return []\n",
    "\n",
    "#     idx = row.indices\n",
    "#     data = row.data\n",
    "#     top = idx[np.argsort(-data)[:k]]\n",
    "\n",
    "#     return feats[top].tolist()\n",
    "\n",
    "\n",
    "# def extract_keywords_tfidf(text: str, top_k: int = 5) -> List[str]:\n",
    "#     tokens = extract_nouns_and_compounds(text)\n",
    "#     if not tokens:\n",
    "#         return []\n",
    "#     return tfidf_top_k([tokens], k=top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fe50f",
   "metadata": {},
   "source": [
    "#LLMã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697f4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords_llm(question: str, max_k: int = 3) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "ä»¥ä¸‹ã®è³ªå•æ–‡ã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ã€åŸºæº–ã€‘\n",
    "- æ•°å­—ã‚’å«ã‚€èªã¯å¿…ãšæŠ½å‡º\n",
    "- å°‚é–€ç”¨èªãƒ»å›ºæœ‰åè©ãƒ»çã—ã„èªã‚’å„ªå…ˆ\n",
    "- ä¸€èˆ¬èªï¼ˆã“ã¨ï¼ãŸã‚ï¼å¿…è¦ï¼å¯¾å¿œï¼å®Ÿæ–½ï¼ã‚‚ã®ï¼ã“ã‚Œï¼ãã‚Œç­‰ï¼‰ã¯é™¤å¤–\n",
    "- é•·ã™ãã‚‹èªï¼ˆ10å­—è¶…ï¼‰ã¯åˆ†å‰²ã—ã¦æŠ½å‡º\n",
    "- åŠ©è©ãƒ»æ¥ç¶šè©ã¯é™¤å¤–\n",
    "- æœ€å¤§ {max_k} å€‹\n",
    "- å‡ºåŠ›ã¯ JSON é…åˆ—ã®ã¿ï¼ˆèª¬æ˜ç¦æ­¢ï¼‰\n",
    "\n",
    "ã€è³ªå•ã€‘\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(LLM_URL, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        # JSON ä»¥å¤–ãŒæ··ã˜ã£ã¦ã„ãŸã‚‰é™¤å»ã™ã‚‹å®‰å…¨å‡¦ç†\n",
    "        import json\n",
    "        keywords = json.loads(content)\n",
    "\n",
    "        # å¿µã®ãŸã‚æ–‡å­—åˆ—ã ã‘ã«ã™ã‚‹\n",
    "        keywords = [k for k in keywords if isinstance(k, str)]\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM keyword generation failed:\", e)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e994a9e",
   "metadata": {},
   "source": [
    "#oræ¤œç´¢â†’andæ¤œç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8301b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#andæ¤œç´¢\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "# def keyword_mask_on_chunks(df_chunks: pd.DataFrame, keywords):\n",
    "#     if not keywords:\n",
    "#         return np.ones(len(df_chunks), dtype=bool)\n",
    "\n",
    "#     s = df_chunks[\"chunk_text\"].fillna(\"\")\n",
    "#     mask = np.ones(len(df_chunks), dtype=bool)\n",
    "\n",
    "#     # ã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã ã‘ Trueï¼ˆAND æ¤œç´¢ï¼‰\n",
    "#     for kw in keywords:\n",
    "#         mask &= s.str.contains(re.escape(kw), regex=True)\n",
    "\n",
    "#     return mask\n",
    "\n",
    "# oræ¤œç´¢\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def keyword_mask_on_chunks(df_chunks: pd.DataFrame, keywords):\n",
    "    if not keywords:\n",
    "        return np.ones(len(df_chunks), dtype=bool)\n",
    "    pattern = \"|\".join(map(re.escape, keywords))\n",
    "    return df_chunks[\"chunk_text\"].fillna(\"\").str.contains(pattern, regex=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4194ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "def build_faiss_ip(embs: np.ndarray):\n",
    "    if embs.size == 0:\n",
    "        return None\n",
    "    d = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)     # normalize_embeddings=True ãªã®ã§å†…ç©=ã‚³ã‚µã‚¤ãƒ³\n",
    "    index.add(embs.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "def vector_search_in_filtered_chunks(df_chunks, embs, encoder, query, pre_mask=None, keywords=None, top_k=5):\n",
    "    # ã©ã¡ã‚‰ã‹æŒ‡å®šï¼špre_maskå„ªå…ˆ\n",
    "    if pre_mask is None:\n",
    "        pre_mask = keyword_mask_on_chunks(df_chunks, keywords or [])\n",
    "    if pre_mask.sum() == 0:\n",
    "        return df_chunks.head(0).assign(score=[])\n",
    "\n",
    "    df_f = df_chunks.loc[pre_mask].reset_index(drop=True)\n",
    "    embs_f = embs[pre_mask]\n",
    "\n",
    "    index = build_faiss_ip(embs_f)\n",
    "    if index is None:\n",
    "        return df_chunks.head(0).assign(score=[])\n",
    "\n",
    "    q = encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "    scores, idxs = index.search(q, top_k)\n",
    "    scores, idxs = scores[0], idxs[0]\n",
    "\n",
    "    hits = []\n",
    "    for rank, (i_sub, s) in enumerate(zip(idxs, scores), 1):\n",
    "        if i_sub < 0:\n",
    "            continue\n",
    "        row = df_f.iloc[int(i_sub)].copy()\n",
    "        row[\"rank\"] = rank\n",
    "        row[\"score\"] = float(s)\n",
    "        hits.append(row)\n",
    "    return pd.DataFrame(hits).sort_values(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193deaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š chunks=12239, embs=(12239, 384)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼†ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "def load_index():\n",
    "    df_chunks = pd.read_parquet(\"chunks_all-MiniLM-L6-v2.parquet\")\n",
    "    embs = np.load(\"embeddings_all-MiniLM-L6-v2.npy\")\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    print(f\"ğŸ“š chunks={len(df_chunks)}, embs={embs.shape}\")\n",
    "    return df_chunks, embs, encoder\n",
    "\n",
    "df_chunks, embs, encoder = load_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e570e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(question: str, top_k: int = 0):\n",
    "    # 1) è³ªå•å‰å‡¦ç†\n",
    "    q = apply_thesaurus(question)\n",
    "    q = preprocess_question_with_hyde_then_tokens(q)\n",
    "\n",
    "\n",
    "\n",
    "    #æ—§ï¼šTF-IDF\n",
    "    #keywords = extract_keywords_tfidf(q, top_k=3)\n",
    "    # æ–°ï¼šLLMãƒ™ãƒ¼ã‚¹\n",
    "    keywords = generate_keywords_llm(q, max_k=3)\n",
    "    #keywords = [\"æ®‹ç•™ç†±é™¤å»ç³»\"]\n",
    "\n",
    "\n",
    "    print(\"ğŸ“ question:\", question)\n",
    "    print(\"ğŸ“ question (HyDEå¾Œ):\", q)\n",
    "    print(\"ğŸ”‘ keywords:\", keywords)\n",
    "\n",
    "    # 2) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒãƒ£ãƒ³ã‚¯çµã‚Šè¾¼ã¿\n",
    "    mask = keyword_mask_on_chunks(df_chunks, keywords)\n",
    "    print(f\"ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒãƒ£ãƒ³ã‚¯æ•°: {mask.sum()} / {len(df_chunks)}\")\n",
    "\n",
    "    # 3) ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢\n",
    "    results = vector_search_in_filtered_chunks(\n",
    "        df_chunks=df_chunks,\n",
    "        embs=embs,\n",
    "        encoder=encoder,\n",
    "        query=q,\n",
    "        pre_mask=mask,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    cols = [\"rank\", \"score\", \"id\", \"midashi_1\", \"doc_idx\", \"chunk_id\", \"chunk_text\", \"kaisai_date\"]\n",
    "    return results[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b755d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def build_context_from_results(results: pd.DataFrame, max_chars: int = 4000) -> str:\n",
    "    \"\"\"\n",
    "    search() ã®çµæœ DataFrame ã‹ã‚‰ã€LLM ã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’ä½œã‚‹ã€‚\n",
    "    é•·ããªã‚Šã™ããªã„ã‚ˆã†ã« max_chars ã§æ‰“ã¡åˆ‡ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total_len = 0\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        header = f\"[doc_idx={row.get('doc_idx')}, chunk_id={row.get('chunk_id')}, title={row.get('title_1', '')}]\"\n",
    "        body = str(row[\"chunk_text\"])\n",
    "        block = header + \"\\n\" + textwrap.fill(body, 80) + \"\\n\"\n",
    "\n",
    "        if total_len + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total_len += len(block)\n",
    "\n",
    "    if not parts:\n",
    "        return \"(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ)\"\n",
    "\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "\n",
    "def answer_with_rag(question: str, top_k: int = 5, max_context_chars: int = 4000):\n",
    "    \"\"\"\n",
    "    ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆLLMjpï¼‰ã‚’åˆ©ç”¨ã—ãŸ RAG å›ç­”ç”Ÿæˆã€‚\n",
    "    1. search() ã§é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—\n",
    "    2. ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã« LLMjp ã§å›ç­”ã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    # 1) ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢\n",
    "    results = search(question, top_k=top_k)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return \"é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\", results\n",
    "\n",
    "    # 2) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ„ã¿ç«‹ã¦\n",
    "    context = build_context_from_results(results, max_chars=max_context_chars)\n",
    "\n",
    "    system_prompt = \"\"\"ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹åŸå­åŠ›ãƒ»å·¥å­¦ç³»ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n",
    "ä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã«å«ã¾ã‚Œã‚‹æƒ…å ±ã‚’æœ€å„ªå…ˆã—ã¦ä½¿ã„ã€\n",
    "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ã„ã¦ã„ãªã„ã“ã¨ã¯æ†¶æ¸¬ã§è£œã‚ãšã€\n",
    "ãã®å ´åˆã¯ã€Œè³‡æ–™ã‹ã‚‰ã¯èª­ã¿å–ã‚Œã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"# è³ªå•\n",
    "{question}\n",
    "\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ¤œç´¢çµæœï¼‰\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "    # LLMjpã‚’å‘¼ã¶\n",
    "    answer = llm_chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return answer.strip(), results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37e5ca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ question: \n",
      "ï½Šï½’ï½’ï¼ï¼“ã®åŸå­ç‚‰æ–½è¨­ã®è¨­ç½®ç”³è«‹ã«ã‚ãŸã‚Šã€ã©ã®ç¨‹åº¦ã®åœ°éœ‡ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã‹ã€‚ç°¡æ½”ã«æ•™ãˆã¦ã€‚\n",
      "\n",
      "ğŸ“ question (HyDEå¾Œ): \n",
      "ï½Šï½’ï½’ï¼ï¼“ã®åŸå­ç‚‰æ–½è¨­ã®è¨­ç½®ç”³è«‹ã«ã‚ãŸã‚Šã€ã©ã®ç¨‹åº¦ã®åœ°éœ‡ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã‹ã€‚ç°¡æ½”ã«æ•™ãˆã¦ã€‚\n",
      "\n",
      "ğŸ”‘ keywords: ['jr', '3', 'åœ°éœ‡']\n",
      "ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒãƒ£ãƒ³ã‚¯æ•°: 3335 / 12239\n",
      "â–¼ å›ç­”\n",
      "\n",
      "è³‡æ–™ã‹ã‚‰ã¯èª­ã¿å–ã‚Œã¾ã›ã‚“ã€‚\n",
      "\n",
      "â–¼ å‚ç…§ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ï¼ˆä¸Šä½ï¼‰\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>midashi_1</th>\n",
       "      <th>doc_idx</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>kaisai_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>1</td>\n",
       "      <td>0.727680</td>\n",
       "      <td>37418</td>\n",
       "      <td>äº‹æ¥­è€…ã¨ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¦‚è¦ãƒ»è³‡æ–™</td>\n",
       "      <td>645</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>2</td>\n",
       "      <td>0.712691</td>\n",
       "      <td>37427</td>\n",
       "      <td>äº‹æ¥­è€…ã¨ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¦‚è¦ãƒ»è³‡æ–™</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>å¹³æˆ26å¹´08æœˆ19æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>3</td>\n",
       "      <td>0.691155</td>\n",
       "      <td>36763</td>\n",
       "      <td>å¹³æˆ25å¹´11æœˆ22æ—¥</td>\n",
       "      <td>446</td>\n",
       "      <td>0</td>\n",
       "      <td>å¹³æˆ25å¹´11æœˆ22æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>4</td>\n",
       "      <td>0.678531</td>\n",
       "      <td>35792</td>\n",
       "      <td>ç¬¬118å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>å¹³æˆ26å¹´6æœˆ13æ—¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>5</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>35645</td>\n",
       "      <td>æ³Šç™ºé›»æ‰€ã€€ï¼“å·ç‚‰ã€€é–¢é€£å¯©æŸ»ä¼šåˆã€€å¹³æˆ25å¹´åº¦</td>\n",
       "      <td>182</td>\n",
       "      <td>127</td>\n",
       "      <td>å¹³æˆ26å¹´2æœˆ18æ—¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rank     score     id                    midashi_1  doc_idx  chunk_id  \\\n",
       "3282     1  0.727680  37418              äº‹æ¥­è€…ã¨ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¦‚è¦ãƒ»è³‡æ–™      645         0   \n",
       "3284     2  0.712691  37427              äº‹æ¥­è€…ã¨ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¦‚è¦ãƒ»è³‡æ–™      649         0   \n",
       "3254     3  0.691155  36763                  å¹³æˆ25å¹´11æœˆ22æ—¥      446         0   \n",
       "2470     4  0.678531  35792  ç¬¬118å›åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆ      218         0   \n",
       "2032     5  0.678223  35645       æ³Šç™ºé›»æ‰€ã€€ï¼“å·ç‚‰ã€€é–¢é€£å¯©æŸ»ä¼šåˆã€€å¹³æˆ25å¹´åº¦      182       127   \n",
       "\n",
       "      kaisai_date  \n",
       "3282         None  \n",
       "3284  å¹³æˆ26å¹´08æœˆ19æ—¥  \n",
       "3254  å¹³æˆ25å¹´11æœˆ22æ—¥  \n",
       "2470   å¹³æˆ26å¹´6æœˆ13æ—¥  \n",
       "2032   å¹³æˆ26å¹´2æœˆ18æ—¥  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "ï½Šï½’ï½’ï¼ï¼“ã®åŸå­ç‚‰æ–½è¨­ã®è¨­ç½®ç”³è«‹ã«ã‚ãŸã‚Šã€ã©ã®ç¨‹åº¦ã®åœ°éœ‡ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã‹ã€‚ç°¡æ½”ã«æ•™ãˆã¦ã€‚\n",
    "\"\"\"\n",
    "\n",
    "answer, used_chunks = answer_with_rag(q, top_k=5)\n",
    "\n",
    "print(\"â–¼ å›ç­”\\n\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nâ–¼ å‚ç…§ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ï¼ˆä¸Šä½ï¼‰\\n\")\n",
    "display(used_chunks[[\"rank\", \"score\", \"id\", \"midashi_1\", \"doc_idx\", \"chunk_id\", \"kaisai_date\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2ca612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- rank 1 (doc_idx=645, chunk_id=0) ----\n",
      "ï¼–ï¼æå‡ºè³‡æ–™ãƒ»æµœå²¡åŸå­åŠ›ç™ºé›»æ‰€ï¼”å·ç‚‰æ•·åœ°ã«ãŠã‘ã‚‹åœ°éœ‡å‹•ã®å¢—å¹…ç‰¹æ€§ã«ã¤ã„ã¦ï¼ï¼’ï¼\n",
      "\n",
      "---- rank 2 (doc_idx=649, chunk_id=0) ----\n",
      "â‘¢ä¸­éƒ¨é›»åŠ›ã‹ã‚‰ã€æœ¬æ—¥ã®æŒ‡æ‘˜ç­‰ã«ã¤ã„ã¦äº†è§£ã—ãŸæ—¨ã®å›ç­”ãŒã‚ã£ãŸã€‚ï¼–ï¼æå‡ºè³‡æ–™ãƒ»æµœå²¡åŸå­åŠ›ç™ºé›»æ‰€ï¼”å·ç‚‰æ•·åœ°ã«ãŠã‘ã‚‹åœ°éœ‡å‹•ã®å¢—å¹…ç‰¹æ€§ã«ã¤ã„ã¦ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆå›ç­”ï¼‰ãƒ»é˜²æ³¢å ¤ç­‰ã®å„æ–½è¨­ã®è¨­è¨ˆã«ãŠã‘ã‚‹åŸºæº–åœ°éœ‡å‹•ã®ä½¿ã„åˆ†ã‘ã«ã¤ã„ã¦ï¼ï¼’ï¼\n",
      "\n",
      "---- rank 3 (doc_idx=446, chunk_id=0) ----\n",
      "ç‚¹ã§å–å¾—ã•ã‚Œã¦ã„ã‚‹è¦³æ¸¬è¨˜éŒ²ç­‰ã‚’ç”¨ã„ã¦è©•ä¾¡ã‚’é€²ã‚ã‚‹ã“ã¨ã€‚â‘¢ä¹å·é›»åŠ›ã‹ã‚‰ã€æœ¬æ—¥ã®æŒ‡æ‘˜ç­‰ã«ã¤ã„ã¦äº†è§£ã—ãŸæ—¨ã®å›ç­”ãŒã‚ã£ãŸã€‚ï¼–ï¼æå‡ºè³‡æ–™ãƒ»å·å†…åŸå­åŠ›ç™ºé›»æ‰€ãƒ»ç„æµ·åŸå­åŠ›ç™ºé›»æ‰€éœ‡æºã‚’ç‰¹å®šã›ãšç­–å®šã™ã‚‹åœ°éœ‡å‹•ã«ã¤ã„ã¦ï¼ˆç¬¬ï¼”ï¼”å›å¯©æŸ»ä¼šåˆã§ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¸ã¾ãˆãŸæ¤œè¨çŠ¶æ³ï¼‰ï¼ï¼’ï¼\n",
      "\n",
      "---- rank 4 (doc_idx=218, chunk_id=0) ----\n",
      "åŸå­åŠ›ç™ºé›»æ‰€ã®æ–°è¦åˆ¶åŸºæº–é©åˆæ€§ã«ä¿‚ã‚‹å¯©æŸ»ä¼šåˆï¼ˆç¬¬ï¼‘ï¼‘ï¼˜å›ï¼‰ï¼‘ï¼æ—¥æ™‚å¹³æˆï¼’ï¼–å¹´ï¼–æœˆï¼‘ï¼“æ—¥ï¼ˆé‡‘ï¼‰ï¼‘ï¼“ï¼šï¼“ï¼ï½ï¼‘ï¼—ï¼šï¼“ï¼ï¼’ï¼å ´æ‰€åŸå­åŠ›è¦åˆ¶å§”å“¡ä¼šï¼‘ï¼“ï½†ä¼šè­°å®¤ï½ï¼“ï¼è­°é¡Œï¼ˆï¼‘ï¼‰åœ°éœ‡ã«ã¤ã„ã¦ï¼ˆï¼’ï¼‰ãã®ä»–ï¼”ï¼é…ä»˜è³‡æ–™è³‡æ–™ï¼‘é«˜æµœç™ºé›»æ‰€ï¼“ãƒ»ï¼”å·æ©ŸåŸå­ç‚‰å»ºå±‹ä»–ã®åŸºç¤åœ°ç›¤åŠã³å‘¨è¾ºæ–œé¢ã®å®‰å®šæ€§è©•ä¾¡ã«ã¤ã„ã¦\n",
      "\n",
      "---- rank 5 (doc_idx=182, chunk_id=127) ----\n",
      "åœ°éœ‡ãƒ»æ´¥æ³¢é–¢ä¿‚ã®å¯©æŸ»ä¼šåˆã‚’é–‹å‚¬ã™ã‚‹äºˆå®šã«ãªã£ã¦ã„ã¾ã™ã€‚ãã‚Œã§ã¯ã€ä»¥ä¸Šã§æœ¬æ—¥ã®å¯©è­°ã‚’çµ‚äº†ã„ãŸã—ã¾ã™ã€‚ï¼‘ï¼’ï¼”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# used_chunks ã« doc_idx, chunk_id ãŒå…¥ã£ã¦ã„ã‚‹å‰æ\n",
    "inspect_df = (\n",
    "    used_chunks[[\"rank\", \"score\", \"doc_idx\", \"chunk_id\"]]\n",
    "    .merge(\n",
    "        chunks[[\"doc_idx\", \"chunk_id\", \"chunk_text\"]],\n",
    "        on=[\"doc_idx\", \"chunk_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "for _, row in inspect_df.iterrows():\n",
    "    print(f'---- rank {row[\"rank\"]} (doc_idx={row[\"doc_idx\"]}, chunk_id={row[\"chunk_id\"]}) ----')\n",
    "    print(row[\"chunk_text\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83006d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- target chunk ----\n",
      "ã¾ã™ã€‚ã“ã‚Œã‚‚è¡¨ã«ãªã£ã¦ã”ã–ã„ã¾ã—ã¦ã€ï¼—æ—¥é–“ã§ä½¿ã„ã¾ã™ç‡ƒæ–™ã‚’å…¨ã¦åˆè¨ˆã—ã¦ã”ã–ã„ã¾ã™ã€‚æ¬„ã®ä¸€ç•ªå·¦å´ãŒç§»å‹•å¼å¤§å®¹é‡ç™ºé›»æ©Ÿã€ãã‚Œã‹ã‚‰ãã®æ¬¡ã®æ¬„ãŒä½¿ç”¨æ¸ˆç‡ƒæ–™ãƒ”ãƒƒãƒˆå¾©æ°´ã‚¿ãƒ³ã‚¯è£œçµ¦ç”¨ã®æ°´ä¸­ãƒãƒ³ãƒ—ç”¨ã®ç™ºé›»æ©Ÿã€æ°´ä¸­ãƒãƒ³ãƒ—ã¯ã€ãƒãƒ³ãƒ—ã«é›»æºã‚’ã¤ãªã„ã§ã‚„ã‚‹ã‚“ã§ã™ã‘ã©ã‚‚ã€ãã®é›»æºã¯ç™ºé›»æ©Ÿè»Šã€ã„ã‚ã‚†ã‚‹ä½œæ¥­å·¥äº‹ç¾å ´ã«ã‚ã‚Šã¾ã™ã‚ˆã†ãªãƒãƒ¼ã‚¿ãƒ–ãƒ«ãªç™ºé›»æ©Ÿã‚’ã€å››è§’ãã¦ã¡ã‚‡ã£ã¨å¤§ãç›®ã®ã‚„ã¤ã§ã™ã‘ã©ã‚‚ã€é…å‚™ã—ã¾ã—ã¦ã‚„ã‚Šã¾ã™ã€‚ãã®ç™ºé›»æ©Ÿã«ç‡ƒæ–™ãŒå¿…è¦ã§ã™ã®ã§ã€ãã®ç‡ƒæ–™ã®é‡ã‚’è¨ˆç®—ã—ã¾ã—ãŸã®ãŒã“ã®æ¬„ã§ã”ã–ã„ã¾ã™ã€‚ãã‚Œã‹ã‚‰ã€ç§»å‹•å¼ã®å¤§å®¹é‡ãƒãƒ³ãƒ—è»Šã€ãã‚Œã‹ã‚‰ãã®å³å´ã§ã™ã‘ã©ã‚‚ã€å…ˆã»ã©ã¯å¾©æ°´ã‚¿ãƒ³ã‚¯ã®çµ¦æ°´ç”¨ã®æ°´ä¸­ãƒãƒ³ãƒ—ã€ãã‚Œã‹ã‚‰ä¸€ç•ªå³å´ã®æ¬„ãŒä½¿ç”¨æ¸ˆç‡ƒæ–™ãƒ”ãƒƒãƒˆã¸ã®çµ¦æ°´ç”¨ã®æ°´ä¸­ãƒãƒ³ãƒ—ã§ã€ã“ã‚Œã‚‰ã®ï¼—æ—¥é–“ã§å¿…è¦ãªæ²¹ã®é‡ã‚’åˆè¨ˆã—ã¾ã™ã¨ã€åˆè¨ˆã®æ¬„ã§ã”ã–ã„ã¾ã™ã‘ã©ã‚‚ã€ï¼’ï¼–ä¸‡ï¼–ï¼Œï¼–ï¼ï¼”ï½Œã§ã”ã–ã„ã¾ã—ã¦ã€å‚™è“„ã—ã¦ã„ã‚‹æ²¹ã®é‡ãŒï¼•ï¼‘ä¸‡ï¼˜ï¼Œï¼•ï¼ï¼ï½Œã¨ã„ã†ã“ã¨ã§ã€ååˆ†æ²¹ã®é‡ãŒã‚ã‚‹ã¨ã„ã†ã“ã¨ãŒç¢ºèªã§ãã¦ã„ã¾ã™ã€‚ï¼‘å·æ©Ÿã€ï¼’å·æ©Ÿã¨ã‚‚åŒæ§˜ã§ã”ã–ã„ã¾ã™ã€‚ãã‚Œã‹ã‚‰ã€æ¬¡ã®æ¬„ãŒâ€¦â€¦ã€‚â—‹æ›´ç”°å§”å“¡ã™ã¿ã¾ã›ã‚“ã€æ™‚é–“ã®é–¢ä¿‚ãŒã‚ã‚‹ã®ã§ã€ï¼‘å›ã“ã“ã§åˆ‡ã‚‰ã›ã¦ãã ã•ã„ã€‚â—‹ä¹å·é›»åŠ›ï¼ˆé ˆè—¤ï¼‰ã¯ã„ã€‚â—‹æ›´ç”°å§”å“¡è³‡æ–™ï¼’ï¼ï¼‘ã€ï¼’ï¼ï¼’ã‚‚ä½¿ã„ãªãŒã‚‰ã§ã™ã‘ã©ã‚‚ã€è³‡æ–™ï¼’ï¼ï¼‘ã§ã“ã“ã¾ã§èª¬æ˜ã—ã¦ã„ãŸã ã„ãŸç‚¹ã«ã¤ã„ã¦ã€å°‘ã—ä¸Šã‹ã‚‰ã¨æ€ã„ã¾ã™ã‘ã©ã€é‡è¦äº‹æ•…ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«é–¢ã—ã¦ã¯ã€ï¼‘æ¬¡ç³»å†·å´æãƒãƒ³ãƒ—ã‚·ãƒ¼ãƒ«éƒ¨ã‹ã‚‰ã®å†·å´æå–ªå¤±ã‚’ä¼´ã†ï½“ï½‚ï½ã®éš›ã«ï¼‘æ¬¡å†·å´æã®æµå‡ºã‚’ä¼´ã†ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¤ã„ã¦ã§ã™ã‘ã‚Œã©ã‚‚ã€ã¾ãšã‚·ãƒŠãƒªã‚ªã¨æœ€åˆã®æ¡ä»¶ç­‰ã€…ã‹ã‚‰ã ã¨æ€ã„ã¾ã™ã‘ã©ã‚‚ã€è³ªå•ãŒï¼’ï¼—ã‚ã‚Œã°ã€‚â—‹æµ¦é‡èª¿æ•´å®˜å®‰å…¨è¦åˆ¶èª¿æ•´å®˜ã€å·å†…åŸå­åŠ›ç™ºé›»æ‰€ï¼‘ãƒ»ï¼’å·æ©Ÿã‚’æ‹…å½“ã—ã¦ã„ã¾ã™ã€æµ¦é‡ã§ã”ã–ã„ã¾ã™ã€‚æ¡ä»¶ã®ã¨ã“ã‚ã§ã”ã–ã„ã¾ã™ã‘ã‚Œã©ã‚‚ã€ãŸã ã„ã¾å¾¡èª¬æ˜ã®ï¼•ãƒšãƒ¼ã‚¸ã®ã¨ã“ã‚ã®ç‚‰å¿ƒå´©å£Šç†±ã¨ã„ã†ã“ã¨ã§ã€ã“ã“ã§ã¯æ¡ä»¶è¨­å®šã®è€ƒãˆæ–¹ã®ã¿ã®å¾¡èª¬æ˜ã«ã¡ã‚‡ã£ã¨ã¨ã©ã¾ã£ã¦ãŠã‚Šã¾ã—ã¦ã€å®Ÿéš›ã«ã©ã®ã‚ˆã†ãªã‚¿ã‚¤ãƒ—ã®ç‡ƒæ–™ãŒè£…è·ã•ã‚Œã¦ãŠã£ã¦ã€ãã‚ŒãŒå–æ›¿ç‚‰å¿ƒã¨ã—ã¦ã©ã®ã‚ˆã†ãªçµ„ã¿åˆã‚ã›ã«ãªã£ã¦ã„ã‚‹ã€ãã‚ŒãŒæœ«æœŸã¨ãªã£ã¦ã„ã‚‹å€¤ã¨ã—ã¦ã©ã®ã‚ˆã†ãªå€¤ã¨ãªã£ã¦ã„ã‚‹ã®ã‹ã¨ã€ã“ã‚Œã¯å¾Œã«å‡ºã¦ãã¾ã™ç™ºç”Ÿã™ã‚‹å´©å£Šç†±ã¨ç†±é™¤å»ã®ãƒãƒ©ãƒ³ã‚¹ã®ã‚°ãƒ©ãƒ•ãŒã”ã–ã„ã¾ã™ã‘ã‚Œã©ã‚‚ã€ãã†ã„ã£ãŸã¨ã“ã¨ã‚‚é–¢ä¿‚ã—ã¾ã™ã®ã§ã€å…·ä½“çš„ãªèª¬æ˜ã¯ãœã²ã—ã¦ã„ãŸã ããŸã„ã¨æ€ã†ã®ã§ã™ãŒã€ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ã€‚â—‹ä¹å·é›»åŠ›ï¼ˆç• åŸœï¼‰ç™ºé›»æœ¬éƒ¨ã®ç• åŸœã§ã”ã–ã„ã¾ã™ã€‚äº†è§£ã„ãŸã—ã¾ã—ãŸã€åˆ¥\n"
     ]
    }
   ],
   "source": [
    "target = chunks[\n",
    "    (chunks[\"doc_idx\"] == 103) & (chunks[\"chunk_id\"] == 26)\n",
    "][\"chunk_text\"].iloc[0]\n",
    "\n",
    "print(\"---- target chunk ----\")\n",
    "print(target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
